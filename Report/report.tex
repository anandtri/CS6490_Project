\documentclass[letterpaper,twocolumn,10pt]{article}

%\usepackage{epsfig,xspace,url}
%\usepackage{epstopdf}
%\usepackage{color}
\usepackage{graphicx}
\usepackage{mathptmx}
\usepackage[hidelinks]{hyperref}
\usepackage{xcolor}
\hypersetup{
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}

\title{\vspace{-2.0cm}Client Side Web Cloaking Detection}
\author{
Anand Tripathi\\
       atripath@cs.utah.edu
\and
Kirk Webb\\
       kwebb@cs.utah.edu
\and
Anmol Vatsa\\
       anvatsa@cs.utah.edu
}
\date{Code link: \url{https://github.com/anandtri/CS6490\_Project}\\
\today}

\begin{document}

\maketitle

\section{Introduction}
Web cloaking refers to the set of techniques that a web server uses to fingerprint incoming visitors in order to customize page content. An example can be, a website serving optimized pages for small screens. However, cloaking is often misused by miscreants to hide the true nature of malicious sites.  This may be done to improve search ranking (by stuffing crawler-facing pages with keywords), or simply to hide questionable or malicious content from any other than the intended end-user audience.

Invernizzi et al ~\cite{cloak} measured the prevalence of cloaking on the web as well as its use by attackers to increase their search ranking and hide malicious content being served from web crawlers. The work also proposed a cloaking detection system but it's currently targeted towards search engines and security companies. One interesting insight of the study was that the content of cloaked websites retrieved from the client browser will be different from the one observed with a web crawler.

The goal of this project is to investigate the possibility of detecting web cloaking from a web client vantage point.  We built a browser plug-in that works with other components to detect the use of cloaking by websites; the plug-in blocks and alerts the user on detection. Separately-running crawler proxy and content classification components are invoked by the plug-in.  Figure~\ref{fig:arch} provides a high-level view of our architecture.

%%% ARCHITECTURE FIGURE
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.4\textwidth]{./proj-diagram.png}
  \caption{Website cloak detection components.}
  \label{fig:arch}
\end{figure}

We started with the assumption that cloaked websites serve different content to browser and web crawler.  The browser calls the classifier module, passing it the content of the page.  The classifier makes a parallel crawler request. After retrieving the content from these requests, the classifier extracts the relevant content features and does a comparison ~\cite{cloak} to identify any major differences.  Mismatches over a threshold will be detected as cloaking.  In either case, the classifier returns the detection result back to the browser.

We obtained web sites to test from two sources: web search results and spam email.  These two sources showed divergent techniques for presenting users with web content.  Sites from spam email tended to hide behind HTTP redirect chains while sites from web searches used the form of cloaking previously described.  We wrote our components to extract redirection chains and to be able to classify both types of content hiding.

Our results show that cloaking on specific keywords and topics is very common, despite being heavily penalized and discouraged by search engine providers. Over 50\% of all results in the top 50(?) list of sites returned in a Google search for ``buy viagra'' were cloaked. Current trends in sites provided in email nearly all use redirection chains in an attempt to hide the final site contents from casual scanning and crawling.  Operators of cloaked websites want users to find their content using search engines.  Sites delivered via spam email only want to be located via the spam itself.

\section{Related Work}
Our work is mostly inspired by `Cloak of Visibility' by Invernizzi et al.\cite{cloak}. But many other works discuss cloaking on similar lines. Baoning Wu and Brian D. Davison in their paper on `Detecting semantic cloaking on the web'\cite{Wu:2006:DSC:1135777.1135901} use a very similar approach as the one we used in this project, i.e. matching different copies of the same page fetched via a browser and a proxy crawler. However, they go a step farther by establishing a two step process that uses a heuristic to match the content/context of the different versions of the pages by extracting keywords from the pages. This helps them eliminate some of the false positives where relatively similar, but dynamic content is served every time the page is opened.\\
X Liao et al.\cite{liao2016lurking} do a comprehensive analysis of public cloud repositories to find out malicious cloud buckets that serve as gateways, content repositories or nodes in redirection based cloaking techinques. Part of their work focuses on identifying redirection based obfuscation based on the analysis of the entire redirection chain, which could be a most interesting addition to our own attempt at web cloak detection.

`Cloak and Dagger: Dynamics of Web Search Cloaking'\cite{Wang:2011:CDD:2046707.2046763} takes up the study of Web Cloaking over a period of time, tracking popular search keywords and terms over a period of 5 months and determining the expected lifetime of cloaked search results using their custom crawler.

A.Naga Venkata Sunila and Anjali Sardana in their work `A Reputation Based Detection Technique to Cloaked Web Spam'\cite{sunil2012reputation} propose a technique of sorting URL's by reputation that could be applied on a crowd-source basis to effectively eliminiate further false positives for highly dynamic but community trusted sites.
\section{Adversary Model}

This work operates under the premise that cloaked websites utilize largely simple techniques to differentiate between browser clients and web crawlers. In particular, we assume that site operator adversaries gate on HTTP client headers such as \texttt{User-Agent}.  Typical browsers and search engines make clear their disposition via these headers.  Our classifier component is able to distinguish differences in relatively static web content.  It does not do well with highly dynamic pages such as news sites.  Further, our crawler proxy is able to handle redirection chains that explicitly use HTTP redirect return codes in addition to obvious javascript-based redirection code snippets.  Our components cannot handle obfuscation that occurs through general javascript code.  Such techniques are most common on sites protected from attack by wrapper services such as CloudFlare~\ref{cloudflare} and CloudFront~\ref{cloudfront}.

\section{Methodology}

The general approach we take to detecting web cloaking is to fetch pages from multiple vantage points and compare the results.  Our component architecture allow for strategic deployment of functionality to take advantage of greater processing resources for classification, or different Internet subnets for collecting website content. Experiments were conducted on personal laptops, on the CloudLab testbed~\cite{cloudlab}, and on Google Cloud's Compute Engine~\cite{gcloud}. The classifier uses lightweight content feature extraction and similarity scoring to detect divergence.  We stepped through a range of score thresholds to find the one that minimizes false positives while maximizing true positives.  Websites where cloaking was detected where manually inspected for verification.

\subsection{Browser Plugin}

The browser plugin is modeled as a Chrome Browser Extension. It utilizes the Chrome Platform Javascript APIs to intercepts all webrequests from the browser, inspect Headers for HTTP redirects and block the request if required. A snapshot of this extension is shown in Figure \ref{fig:plugin}.

%%%CHROME BROWSER EXTENSION
\begin{figure}[t]
  \centering
  \includegraphics[width=0.4\textwidth]{./proj_plugin.png}
  \caption{Chrome Browser Extension Snapshot.}
  \label{fig:plugin}
\end{figure}

Upon intercepting a web request, the browser plugin fetches the page content in a parallel request and invokes the Classifier module. It provides the content of the web page requested to the Classifier module as part of this request. The plugin makes no assumptions about the location of the Classifier module and allows the user/app-developer to configure its location in the plugin as a destination IPaddress and port. The details of Cloak detection are completely abstracted away from the plugin. It completely relies on the Classifier module to do the job, including invoking the Crawler Proxy. It expects a binary yes or no result, along with a reason for the decision. This model would potentially allow easy updates to the backend modules, i.e. the Classifier and the Crawler Proxy modules, without affecting the user facing browser plugin component.

Upon receiving the results from the Classifier, the browser either lets the original web request go through, or warns the user that the URL requested is serving Cloaked Content and prompts the user to cancel the request and add the domain of the URL to its Blacklist. Since some websites employ cloaking to legitimately serve different contents on different types of devices and users, the plugin also allows an option to add domains to a Whitelist, just in case the Classifier module classifies a legitimate cloaked URL as malicious. The browser plugin will not block and inspect any request to any URL on the domains added in the Whitelist.

\subsection{Crawler Proxy}
The crawler proxy operates as a stand-alone daemon that waits for incoming requests to fetch content.  When an incoming RPC asks for a particular URL, the crawler fetches the corresponding page, recording both HTTP headers andthe page body returned by the server resolved in the URL.  If the server returns HTTP redirect (code 301 - 310), the crawler extracts and follows the redirect URL provided.  If the page content includes straightforward code snippets that perform redirect based on the javascript \texttt{onload} event, these are found, extracted, and followed as well.  The full sequence of headers and pages found are returned as a result stream to the caller for analysis.

\section{Implementation and Experimentation}

\section{Conclusion}

{
  \small
  \bibliographystyle{acm}
  \bibliography{biblio}
}

\end{document}
